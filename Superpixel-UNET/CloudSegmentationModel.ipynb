{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Utilities/')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import importlib\n",
    "import data_utils\n",
    "importlib.reload(data_utils)\n",
    "\n",
    "## Import MDS from sklearn\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=1, random_state=0, normalized_stress='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet1D(\n",
      "  (encoder): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(6, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ConvTranspose1d(512, 256, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (logits): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth=2, num_layers=2):\n",
    "        super(UNet1D, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.depth = depth\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.num_start_filters = 32\n",
    "\n",
    "        self._create_unet(self.in_channels, self.num_start_filters)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv1d(self.num_start_filters * 2 ** (self.depth - 1), 2 * self.num_start_filters * 2 ** (self.depth - 1), kernel_size=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.logits = nn.Conv1d(self.num_start_filters, self.out_channels, 1, 1)\n",
    "\n",
    "\n",
    "    def _create_encoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _create_decoder_block(self, in_channels, out_channels):\n",
    "        return nn.ModuleList([nn.ConvTranspose1d(in_channels, in_channels//2, kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()])\n",
    "\n",
    "    def _create_unet(self, in_channels, out_channels):\n",
    "        for _ in range(self.depth):\n",
    "            self.encoder.append(self._create_encoder_block(in_channels, out_channels))\n",
    "            in_channels, out_channels = out_channels, out_channels*2\n",
    "\n",
    "        out_channels = in_channels\n",
    "        in_channels = in_channels * 2\n",
    "        for _ in range(self.depth):\n",
    "            self.decoder.append(self._create_decoder_block(in_channels, out_channels))\n",
    "            in_channels, out_channels = out_channels, out_channels//2\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = []\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            encoded.append(x)\n",
    "            x = nn.MaxPool1d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "        x = self.bottleneck(x)  # Bottleneck layer\n",
    "\n",
    "        for dec in self.decoder:\n",
    "            ## Adding input with encoder concatenation\n",
    "            enc_output = encoded.pop()\n",
    "            x = dec[0](x)\n",
    "            ## Pad the decoder output to match the encoder output\n",
    "            diff = enc_output.shape[2] - x.shape[2]\n",
    "            x = F.pad(x, (diff // 2, diff - diff // 2))\n",
    "            x = torch.cat((enc_output, x), dim=1)\n",
    "            x = dec[1](x)\n",
    "            x = dec[2](x)\n",
    "        ## Add softmax to logits\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        return self.logits(x)\n",
    "\n",
    "input_channels = 6 \n",
    "output_channels = 2\n",
    "depth = 4\n",
    "num_layers = 2\n",
    "\n",
    "model = UNet1D(input_channels, output_channels, depth, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 300])\n",
      "Output shape: torch.Size([1, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "num_superpixels = 300\n",
    "num_features = 6\n",
    "synthetic_data = np.random.rand(num_superpixels, num_features)\n",
    "synthetic_data = torch.tensor(synthetic_data, dtype=torch.float32)\n",
    "\n",
    "#Reshape\n",
    "synthetic_data = synthetic_data.unsqueeze(0).transpose(1, 2)\n",
    "\n",
    "# Pass the synthetic data through the U-Net model\n",
    "with torch.no_grad():\n",
    "    output = model(synthetic_data)\n",
    "\n",
    "print(\"Input shape:\", synthetic_data.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudSegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CloudSegmentationModel, self).__init__()\n",
    "        self.unet = UNet1D(in_channels=6, out_channels=1, depth=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.Sigmoid()(self.unet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:  66561\n",
      "Input shape: torch.Size([1, 6, 300])\n",
      "Output shape: torch.Size([1, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "model = CloudSegmentationModel()\n",
    "print(\"Parameters: \",count_parameters(model))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(synthetic_data)\n",
    "\n",
    "print(\"Input shape:\", synthetic_data.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GP65\\anaconda3\\lib\\site-packages\\rasterio\\__init__.py:333: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "patches,mask = data_utils.get_patch(path_to_folders_images = \"../Dataset/Natural_False_Color/\", path_to_folders_labels = \"../Dataset/Entire_scene_gts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4fe6d489e64bc1b2d30ce5ac52eb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i,j in tqdm(list(zip(patches,mask))):\n",
    "    try:\n",
    "        output = data_utils.convert_image_array_to_slic_with_properties(i,j,n_segments=300) \n",
    "\n",
    "        ## Getting the X and y arrays\n",
    "        X_array = np.array([list(list(i.values())[0]) + list(i.values())[1:] for i in output[1]])\n",
    "        y_array = np.array(output[2])\n",
    "\n",
    "        ## Normalizing the X_array columwise\n",
    "        X_array[:,0] = X_array[:,0]/255\n",
    "        X_array[:,1] = X_array[:,1]/255\n",
    "        X_array[:,2] = X_array[:,2]/255\n",
    "        X_array[:,3] = X_array[:,3]/512\n",
    "        X_array[:,4] = X_array[:,4]/512\n",
    "        X_array[:,5] = X_array[:,5]/1000\n",
    "\n",
    "\n",
    "\n",
    "        ## Pad the X_array with -1 and y_array with 0 upto 300\n",
    "        X_array = np.pad(X_array,((0,300-X_array.shape[0]),(0,0)),mode='constant',constant_values=-1)\n",
    "        y_array = np.pad(y_array,(0,300-y_array.shape[0]),mode='constant',constant_values=0)\n",
    "\n",
    "        ## Ordering\n",
    "        ordering = mds.fit_transform(X_array[:,3:5]).reshape(-1)\n",
    "        X_array = X_array[ordering.argsort()]\n",
    "        y_array = y_array[ordering.argsort()].reshape(-1,1)\n",
    "\n",
    "        ## Appending\n",
    "        X.append(X_array)\n",
    "        y.append(y_array)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../Dataset/X.npy', np.array(X))\n",
    "# np.save('../Dataset/Y.npy', np.array(y))\n",
    "\n",
    "X = np.load('../Dataset/X.npy')\n",
    "y = np.load('../Dataset/Y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_item = torch.tensor(self.X[idx], dtype=torch.float32).T\n",
    "        y_item = torch.tensor(self.y[idx], dtype=torch.float32).T\n",
    "        \n",
    "        # ## Stack y \n",
    "        # y_item = torch.stack([y_item, 1-y_item], dim=0)\n",
    "        # ## Remove dimension 2\n",
    "        # y_item = torch.squeeze(y_item)\n",
    "        \n",
    "        return x_item, y_item\n",
    "\n",
    "def create_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    dataset = CustomDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide X and Y into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(X_train,y_train,batch_size=64,shuffle=True)\n",
    "test_loader = create_dataloader(X_test,y_test,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        # BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a995742e464990b4cd8c54d4865983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.6364, Accuracy: 0.6186\n",
      "Test Loss: 0.5749, Accuracy: 0.6813\n",
      "Epoch [2/100], Loss: 0.5424, Accuracy: 0.8075\n",
      "Test Loss: 0.5319, Accuracy: 0.7332\n",
      "Epoch [3/100], Loss: 0.5089, Accuracy: 0.7436\n",
      "Test Loss: 0.5077, Accuracy: 0.7726\n",
      "Epoch [4/100], Loss: 0.4980, Accuracy: 0.7191\n",
      "Test Loss: 0.5190, Accuracy: 0.7041\n",
      "Epoch [5/100], Loss: 0.4874, Accuracy: 0.7807\n",
      "Test Loss: 0.5052, Accuracy: 0.7741\n",
      "Epoch [6/100], Loss: 0.4865, Accuracy: 0.8059\n",
      "Test Loss: 0.4932, Accuracy: 0.7659\n",
      "Epoch [7/100], Loss: 0.4706, Accuracy: 0.7999\n",
      "Test Loss: 0.4940, Accuracy: 0.6825\n",
      "Epoch [8/100], Loss: 0.4623, Accuracy: 0.7862\n",
      "Test Loss: 0.4570, Accuracy: 0.7324\n",
      "Epoch [9/100], Loss: 0.4311, Accuracy: 0.7923\n",
      "Test Loss: 0.4193, Accuracy: 0.8353\n",
      "Epoch [10/100], Loss: 0.4135, Accuracy: 0.8057\n",
      "Test Loss: 0.4009, Accuracy: 0.8276\n",
      "Epoch [11/100], Loss: 0.3908, Accuracy: 0.8448\n",
      "Test Loss: 0.3932, Accuracy: 0.8185\n",
      "Epoch [12/100], Loss: 0.3822, Accuracy: 0.8141\n",
      "Test Loss: 0.4353, Accuracy: 0.8000\n",
      "Epoch [13/100], Loss: 0.3903, Accuracy: 0.8330\n",
      "Test Loss: 0.4030, Accuracy: 0.7826\n",
      "Epoch [14/100], Loss: 0.3776, Accuracy: 0.8352\n",
      "Test Loss: 0.3858, Accuracy: 0.8310\n",
      "Epoch [15/100], Loss: 0.3726, Accuracy: 0.8716\n",
      "Test Loss: 0.4051, Accuracy: 0.8147\n",
      "Epoch [16/100], Loss: 0.3773, Accuracy: 0.8135\n",
      "Test Loss: 0.3749, Accuracy: 0.8522\n",
      "Epoch [17/100], Loss: 0.3691, Accuracy: 0.8159\n",
      "Test Loss: 0.3772, Accuracy: 0.8331\n",
      "Epoch [18/100], Loss: 0.3716, Accuracy: 0.8617\n",
      "Test Loss: 0.3750, Accuracy: 0.8180\n",
      "Epoch [19/100], Loss: 0.3648, Accuracy: 0.8038\n",
      "Test Loss: 0.3731, Accuracy: 0.8432\n",
      "Epoch [20/100], Loss: 0.3680, Accuracy: 0.8122\n",
      "Test Loss: 0.3747, Accuracy: 0.8600\n",
      "Epoch [21/100], Loss: 0.3588, Accuracy: 0.8341\n",
      "Test Loss: 0.3594, Accuracy: 0.8796\n",
      "Epoch [22/100], Loss: 0.3608, Accuracy: 0.7990\n",
      "Test Loss: 0.3704, Accuracy: 0.8431\n",
      "Epoch [23/100], Loss: 0.3631, Accuracy: 0.8094\n",
      "Test Loss: 0.3960, Accuracy: 0.8056\n",
      "Epoch [24/100], Loss: 0.3776, Accuracy: 0.7797\n",
      "Test Loss: 0.3749, Accuracy: 0.8666\n",
      "Epoch [25/100], Loss: 0.3576, Accuracy: 0.8825\n",
      "Test Loss: 0.3614, Accuracy: 0.8769\n",
      "Epoch [26/100], Loss: 0.3510, Accuracy: 0.8488\n",
      "Test Loss: 0.3588, Accuracy: 0.8504\n",
      "Epoch [27/100], Loss: 0.3499, Accuracy: 0.8520\n",
      "Test Loss: 0.3615, Accuracy: 0.8409\n",
      "Epoch [28/100], Loss: 0.3496, Accuracy: 0.8516\n",
      "Test Loss: 0.3797, Accuracy: 0.8279\n",
      "Epoch [29/100], Loss: 0.3544, Accuracy: 0.8322\n",
      "Test Loss: 0.3603, Accuracy: 0.8097\n",
      "Epoch [30/100], Loss: 0.3473, Accuracy: 0.8646\n",
      "Test Loss: 0.3570, Accuracy: 0.8493\n",
      "Epoch [31/100], Loss: 0.3488, Accuracy: 0.9014\n",
      "Test Loss: 0.3550, Accuracy: 0.8499\n",
      "Epoch [32/100], Loss: 0.3517, Accuracy: 0.8516\n",
      "Test Loss: 0.3578, Accuracy: 0.8639\n",
      "Epoch [33/100], Loss: 0.3530, Accuracy: 0.8925\n",
      "Test Loss: 0.3826, Accuracy: 0.8382\n",
      "Epoch [34/100], Loss: 0.3535, Accuracy: 0.8435\n",
      "Test Loss: 0.3542, Accuracy: 0.8590\n",
      "Epoch [35/100], Loss: 0.3512, Accuracy: 0.7968\n",
      "Test Loss: 0.3546, Accuracy: 0.8238\n",
      "Epoch [36/100], Loss: 0.3497, Accuracy: 0.8509\n",
      "Test Loss: 0.3586, Accuracy: 0.8158\n",
      "Epoch [37/100], Loss: 0.3487, Accuracy: 0.7954\n",
      "Test Loss: 0.3526, Accuracy: 0.8418\n",
      "Epoch [38/100], Loss: 0.3502, Accuracy: 0.8087\n",
      "Test Loss: 0.3545, Accuracy: 0.8582\n",
      "Epoch [39/100], Loss: 0.3475, Accuracy: 0.8507\n",
      "Test Loss: 0.3495, Accuracy: 0.8665\n",
      "Epoch [40/100], Loss: 0.3413, Accuracy: 0.8607\n",
      "Test Loss: 0.3610, Accuracy: 0.8598\n",
      "Epoch [41/100], Loss: 0.3411, Accuracy: 0.8588\n",
      "Test Loss: 0.3653, Accuracy: 0.8292\n",
      "Epoch [42/100], Loss: 0.3466, Accuracy: 0.7538\n",
      "Test Loss: 0.3583, Accuracy: 0.8170\n",
      "Epoch [43/100], Loss: 0.3496, Accuracy: 0.8058\n",
      "Test Loss: 0.3521, Accuracy: 0.8420\n",
      "Epoch [44/100], Loss: 0.3420, Accuracy: 0.8210\n",
      "Test Loss: 0.3560, Accuracy: 0.8351\n",
      "Epoch [45/100], Loss: 0.3438, Accuracy: 0.8623\n",
      "Test Loss: 0.3527, Accuracy: 0.8623\n",
      "Epoch [46/100], Loss: 0.3405, Accuracy: 0.8481\n",
      "Test Loss: 0.3668, Accuracy: 0.8190\n",
      "Epoch [47/100], Loss: 0.3431, Accuracy: 0.8677\n",
      "Test Loss: 0.3594, Accuracy: 0.8306\n",
      "Epoch [48/100], Loss: 0.3463, Accuracy: 0.8468\n",
      "Test Loss: 0.3480, Accuracy: 0.8651\n",
      "Epoch [49/100], Loss: 0.3455, Accuracy: 0.7896\n",
      "Test Loss: 0.3737, Accuracy: 0.8126\n",
      "Epoch [50/100], Loss: 0.3431, Accuracy: 0.9290\n",
      "Test Loss: 0.3562, Accuracy: 0.8020\n",
      "Epoch [51/100], Loss: 0.3423, Accuracy: 0.8465\n",
      "Test Loss: 0.3484, Accuracy: 0.8579\n",
      "Epoch [52/100], Loss: 0.3405, Accuracy: 0.8293\n",
      "Test Loss: 0.3506, Accuracy: 0.8332\n",
      "Epoch [53/100], Loss: 0.3412, Accuracy: 0.8622\n",
      "Test Loss: 0.3514, Accuracy: 0.8381\n",
      "Epoch [54/100], Loss: 0.3395, Accuracy: 0.9138\n",
      "Test Loss: 0.3631, Accuracy: 0.8173\n",
      "Epoch [55/100], Loss: 0.3398, Accuracy: 0.8867\n",
      "Test Loss: 0.3512, Accuracy: 0.8421\n",
      "Epoch [56/100], Loss: 0.3402, Accuracy: 0.8630\n",
      "Test Loss: 0.3523, Accuracy: 0.8228\n",
      "Epoch [57/100], Loss: 0.3406, Accuracy: 0.8338\n",
      "Test Loss: 0.3515, Accuracy: 0.8315\n",
      "Epoch [58/100], Loss: 0.3405, Accuracy: 0.8123\n",
      "Test Loss: 0.3505, Accuracy: 0.8230\n",
      "Epoch [59/100], Loss: 0.3371, Accuracy: 0.8132\n",
      "Test Loss: 0.3488, Accuracy: 0.8278\n",
      "Epoch [60/100], Loss: 0.3356, Accuracy: 0.8665\n",
      "Test Loss: 0.3484, Accuracy: 0.8620\n",
      "Epoch [61/100], Loss: 0.3388, Accuracy: 0.7607\n",
      "Test Loss: 0.3466, Accuracy: 0.8155\n",
      "Epoch [62/100], Loss: 0.3334, Accuracy: 0.8584\n",
      "Test Loss: 0.3791, Accuracy: 0.7952\n",
      "Epoch [63/100], Loss: 0.3352, Accuracy: 0.8583\n",
      "Test Loss: 0.3614, Accuracy: 0.8063\n",
      "Epoch [64/100], Loss: 0.3375, Accuracy: 0.8059\n",
      "Test Loss: 0.3466, Accuracy: 0.8413\n",
      "Epoch [65/100], Loss: 0.3425, Accuracy: 0.8849\n",
      "Test Loss: 0.3518, Accuracy: 0.8708\n",
      "Epoch [66/100], Loss: 0.3381, Accuracy: 0.8362\n",
      "Test Loss: 0.3440, Accuracy: 0.8552\n",
      "Epoch [67/100], Loss: 0.3365, Accuracy: 0.8391\n",
      "Test Loss: 0.3462, Accuracy: 0.8423\n",
      "Epoch [68/100], Loss: 0.3347, Accuracy: 0.8488\n",
      "Test Loss: 0.3419, Accuracy: 0.8739\n",
      "Epoch [69/100], Loss: 0.3337, Accuracy: 0.8443\n",
      "Test Loss: 0.3564, Accuracy: 0.8506\n",
      "Epoch [70/100], Loss: 0.3369, Accuracy: 0.8955\n",
      "Test Loss: 0.3504, Accuracy: 0.8238\n",
      "Epoch [71/100], Loss: 0.3372, Accuracy: 0.8067\n",
      "Test Loss: 0.3403, Accuracy: 0.8877\n",
      "Epoch [72/100], Loss: 0.3297, Accuracy: 0.9014\n",
      "Test Loss: 0.3503, Accuracy: 0.8435\n",
      "Epoch [73/100], Loss: 0.3399, Accuracy: 0.8719\n",
      "Test Loss: 0.3646, Accuracy: 0.7933\n",
      "Epoch [74/100], Loss: 0.3331, Accuracy: 0.8780\n",
      "Test Loss: 0.3443, Accuracy: 0.8454\n",
      "Epoch [75/100], Loss: 0.3314, Accuracy: 0.8387\n",
      "Test Loss: 0.3448, Accuracy: 0.8392\n",
      "Epoch [76/100], Loss: 0.3346, Accuracy: 0.8351\n",
      "Test Loss: 0.3460, Accuracy: 0.8326\n",
      "Epoch [77/100], Loss: 0.3315, Accuracy: 0.8116\n",
      "Test Loss: 0.3469, Accuracy: 0.8186\n",
      "Epoch [78/100], Loss: 0.3290, Accuracy: 0.8564\n",
      "Test Loss: 0.3455, Accuracy: 0.8140\n",
      "Epoch [79/100], Loss: 0.3299, Accuracy: 0.8400\n",
      "Test Loss: 0.3649, Accuracy: 0.8396\n",
      "Epoch [80/100], Loss: 0.3370, Accuracy: 0.8583\n",
      "Test Loss: 0.3443, Accuracy: 0.8694\n",
      "Epoch [81/100], Loss: 0.3326, Accuracy: 0.8290\n",
      "Test Loss: 0.3439, Accuracy: 0.8352\n",
      "Epoch [82/100], Loss: 0.3278, Accuracy: 0.8742\n",
      "Test Loss: 0.3449, Accuracy: 0.8613\n",
      "Epoch [83/100], Loss: 0.3290, Accuracy: 0.8429\n",
      "Test Loss: 0.3638, Accuracy: 0.8546\n",
      "Epoch [84/100], Loss: 0.3318, Accuracy: 0.8759\n",
      "Test Loss: 0.3491, Accuracy: 0.8474\n",
      "Epoch [85/100], Loss: 0.3278, Accuracy: 0.8246\n",
      "Test Loss: 0.3451, Accuracy: 0.8478\n",
      "Epoch [86/100], Loss: 0.3350, Accuracy: 0.8606\n",
      "Test Loss: 0.3441, Accuracy: 0.8777\n",
      "Epoch [87/100], Loss: 0.3339, Accuracy: 0.8345\n",
      "Test Loss: 0.3442, Accuracy: 0.8825\n",
      "Epoch [88/100], Loss: 0.3273, Accuracy: 0.8371\n",
      "Test Loss: 0.3466, Accuracy: 0.8483\n",
      "Epoch [89/100], Loss: 0.3297, Accuracy: 0.8358\n",
      "Test Loss: 0.3619, Accuracy: 0.8275\n",
      "Epoch [90/100], Loss: 0.3286, Accuracy: 0.8603\n",
      "Test Loss: 0.3437, Accuracy: 0.8082\n",
      "Epoch [91/100], Loss: 0.3276, Accuracy: 0.8488\n",
      "Test Loss: 0.3515, Accuracy: 0.7972\n",
      "Epoch [92/100], Loss: 0.3287, Accuracy: 0.8393\n",
      "Test Loss: 0.3449, Accuracy: 0.8115\n",
      "Epoch [93/100], Loss: 0.3278, Accuracy: 0.8246\n",
      "Test Loss: 0.3428, Accuracy: 0.8436\n",
      "Epoch [94/100], Loss: 0.3260, Accuracy: 0.8228\n",
      "Test Loss: 0.3425, Accuracy: 0.8222\n",
      "Epoch [95/100], Loss: 0.3251, Accuracy: 0.8642\n",
      "Test Loss: 0.3415, Accuracy: 0.8560\n",
      "Epoch [96/100], Loss: 0.3273, Accuracy: 0.7641\n",
      "Test Loss: 0.3389, Accuracy: 0.8261\n",
      "Epoch [97/100], Loss: 0.3258, Accuracy: 0.8796\n",
      "Test Loss: 0.3427, Accuracy: 0.8592\n",
      "Epoch [98/100], Loss: 0.3231, Accuracy: 0.9165\n",
      "Test Loss: 0.3357, Accuracy: 0.8521\n",
      "Epoch [99/100], Loss: 0.3241, Accuracy: 0.8581\n",
      "Test Loss: 0.3477, Accuracy: 0.7904\n",
      "Epoch [100/100], Loss: 0.3243, Accuracy: 0.7816\n",
      "Test Loss: 0.3445, Accuracy: 0.8157\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "segmentationModel = CloudSegmentationModel().to(device)\n",
    "\n",
    "# train_loader = None # Train loader for our dataset\n",
    "# test_loader = None # Test loader for our dataset\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(segmentationModel.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "segmentationModel.train() \n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0\n",
    "\n",
    "    segmentationModel.train()\n",
    "    for superpixel, label in train_loader:\n",
    "        superpixel = superpixel.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = segmentationModel(superpixel)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        ## Calculate accuracy\n",
    "        predicted = torch.round(output)\n",
    "        correct = (predicted == label).sum().item()\n",
    "        total = label.size(0) * label.size(1) * label.size(2)\n",
    "        accuracy = correct / total\n",
    "\n",
    "\n",
    "    # Print epoch statistics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    segmentationModel.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for superpixel, label in test_loader:\n",
    "            superpixel = superpixel.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = segmentationModel(superpixel)\n",
    "            test_loss += criterion(output, label).item()\n",
    "\n",
    "            ## Calculate accuracy\n",
    "            predicted = torch.round(output)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            total = label.size(0) * label.size(1) * label.size(2)\n",
    "            accuracy = correct / total\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    ## Print loss and accuracy\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
