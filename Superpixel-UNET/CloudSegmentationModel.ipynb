{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet1D(\n",
      "  (encoder): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))\n",
      "      (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (logits): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth=2, num_layers=2):\n",
    "        super(UNet1D, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.depth = depth\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.num_start_filters = 32\n",
    "\n",
    "        self._create_unet(self.in_channels, self.num_start_filters)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv1d(self.num_start_filters * 2 ** (self.depth - 1), 2 * self.num_start_filters * 2 ** (self.depth - 1), kernel_size=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.logits = nn.Conv1d(self.num_start_filters, self.out_channels, 1, 1)\n",
    "\n",
    "\n",
    "    def _create_encoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _create_decoder_block(self, in_channels, out_channels):\n",
    "        return nn.ModuleList([nn.ConvTranspose1d(in_channels, in_channels//2, kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()])\n",
    "\n",
    "    def _create_unet(self, in_channels, out_channels):\n",
    "        for _ in range(self.depth):\n",
    "            self.encoder.append(self._create_encoder_block(in_channels, out_channels))\n",
    "            in_channels, out_channels = out_channels, out_channels*2\n",
    "\n",
    "        out_channels = in_channels\n",
    "        in_channels = in_channels * 2\n",
    "        for _ in range(self.depth):\n",
    "            self.decoder.append(self._create_decoder_block(in_channels, out_channels))\n",
    "            in_channels, out_channels = out_channels, out_channels//2\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = []\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            encoded.append(x)\n",
    "            x = nn.MaxPool1d(kernel_size=2, stride=2)(x)\n",
    "\n",
    "        x = self.bottleneck(x)  # Bottleneck layer\n",
    "\n",
    "        for dec in self.decoder:\n",
    "            ## Adding input with encoder concatenation\n",
    "            enc_output = encoded.pop()\n",
    "            x = dec[0](x)\n",
    "            x = torch.cat((enc_output, x), dim=1)\n",
    "            x = dec[1](x)\n",
    "            x = dec[2](x)\n",
    "        return self.logits(x)\n",
    "\n",
    "input_channels = 4  \n",
    "output_channels = 1 \n",
    "depth = 2\n",
    "num_layers = 2\n",
    "\n",
    "model = UNet1D(input_channels, output_channels, depth, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66369\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 4, 3000])\n",
      "Output shape: torch.Size([1, 1, 3000])\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "import numpy as np\n",
    "num_superpixels = 3000\n",
    "num_features = 4\n",
    "synthetic_data = np.random.rand(num_superpixels, num_features)\n",
    "synthetic_data = torch.tensor(synthetic_data, dtype=torch.float32)\n",
    "\n",
    "#Reshape\n",
    "synthetic_data = synthetic_data.unsqueeze(0).transpose(1, 2)\n",
    "\n",
    "# Pass the synthetic data through the U-Net model\n",
    "with torch.no_grad():\n",
    "    output = model(synthetic_data)\n",
    "\n",
    "print(\"Input shape:\", synthetic_data.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudSegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CloudSegmentationModel, self).__init__()\n",
    "        self.unet = UNet1D(in_channels=4, out_channels=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.unet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 4, 3000])\n",
      "Output shape: torch.Size([1, 1, 3000])\n"
     ]
    }
   ],
   "source": [
    "model = CloudSegmentationModel()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(synthetic_data)\n",
    "\n",
    "print(\"Input shape:\", synthetic_data.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     16\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mfor\u001b[39;00m superpixel, label \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     19\u001b[0m         superpixel \u001b[39m=\u001b[39m superpixel\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m         label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segmentationModel = CloudSegmentationModel().to(device)\n",
    "\n",
    "train_loader = None # Train loader for our dataset\n",
    "test_loader = None # Test loader for our dataset\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(segmentationModel.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "segmentationModel.train() \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for superpixel, label in train_loader:\n",
    "        superpixel = superpixel.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = segmentationModel(superpixel)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch statistics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    segmentationModel.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for superpixel, label in test_loader:\n",
    "            superpixel = superpixel.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = segmentationModel(superpixel)\n",
    "            test_loss += criterion(output, label).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
